{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Image\n",
    "We create a docker image to be used to run the `TrainTestClassify.py` script created by [the second notebook](01_Training_Script.ipynb) on the Batch AI cluster.\n",
    "\n",
    "The steps are\n",
    "- [import the libraries](#import),\n",
    "- [create dotenv shared between notebooks](#dotenv)\n",
    "- [create the dockerfile](#dockerfile),\n",
    "- [create the Docker image](#create), and\n",
    "- [test the Docker image](#test).\n",
    "\n",
    "## Import the libraries  <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import shutil\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and import the dotenv <a id='dotenv'></a>\n",
    "Create a new `.env` file to contain names used in multiple notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = os.path.join('.', '.env')  # The location of the dotenv file\n",
    "if os.path.isfile(dotenv_path):          # Remove any pre-existing dotenv file to ensure a blank slate\n",
    "    os.remove(dotenv_path)\n",
    "with open(dotenv_path, 'w'):             # Create an empty dotenv file\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write in your docker login and image repository name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.set_key(dotenv_path, 'docker_login', 'YOUR_DOCKER_LOGIN')\n",
    "dotenv.set_key(dotenv_path, 'image_repo', '/mlbaiht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.set_key(dotenv_path, 'docker_login', 'mabouatmicrosoft')\n",
    "dotenv.set_key(dotenv_path, 'image_repo', '/mlbaiht')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the contents of the `.env` file into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv -o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dockerfile <a id='dockerfile'></a>\n",
    "Create the Docker directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to the directory the requirements file that specifies the Python modules needed to run the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Docker/requirements.txt\n",
    "\n",
    "lightgbm==2.1.2\n",
    "pandas==0.23.4\n",
    "scikit-learn==0.19.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to the directory the dockerfile specifying the build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Docker/Dockerfile\n",
    "\n",
    "# Start from a Python image\n",
    "FROM python:3.5-stretch\n",
    "\n",
    "# Copy into the image the definition of the requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install the requirements\n",
    "RUN python -m pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Docker image <a id='create'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the Docker image that we are creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = os.getenv('docker_login') + os.getenv('image_repo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the image. The first time this is run, this could take almost a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Creating Docker image {}'.format(image_name))\n",
    "!docker build -t $image_name Docker --no-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the image to the docker repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!sudo docker push $image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Docker image <a id='test'></a>\n",
    "We can now test our image with our script locally. The `volume` argument maps the local directory that contains our data and script to `/data` in the container. Then, we call `bash` with a command string that calls Python with a path to the `TrainTestClassifier.py` script and script arguments including the path to the directory that contains the input files. The remaining script arguments are the same as those in the last cell of the [training script creation notebook](http://localhost:8888/notebooks/01_Training_Script.ipynb), and the results should be similar. \n",
    "\n",
    "This should take around five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!docker run --volume $(pwd):/data $image_name python /data/TrainTestClassifier.py --inputs /data --match 5 --estimators 1000 --ngrams 2 --min_child_samples 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [the next notebook](03_Configure_Batch_AI.ipynb), we create a file to contain the Batch AI configuration and some Azure resources we will use to create the Batch AI cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLBatchAIHyperparameterTuning]",
   "language": "python",
   "name": "conda-env-MLBatchAIHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
