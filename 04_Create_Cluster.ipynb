{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cluster\n",
    "Copy the datasets and scripts to the storage, and create the Batch AI cluster in the workspace.\n",
    "\n",
    "The steps are\n",
    "- [import the libraries and dotenv parameters](#import),\n",
    "- [create a Batch AI client](#client),\n",
    "- [copy the scripts and data to Azure storage](#copy), and\n",
    "- [create the Batch AI cluster](#cluster).\n",
    "\n",
    "## Imports <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import dotenv\n",
    "import azure.mgmt.batchai.models as models\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.file import FileService\n",
    "sys.path.append('.')\n",
    "import utilities as utils\n",
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell are the names of various files and services used or created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the dotenv file\n",
    "dotenv_path = dotenv.find_dotenv()\n",
    "# The Azure blob container created for the datasets\n",
    "dotenv.set_key(dotenv_path, 'azure_blob_container_name', 'batchaisample')\n",
    "# The Azure blob container directory containing the datasets\n",
    "dotenv.set_key(dotenv_path, 'dataset_path', 'dataset')\n",
    "# The Azure file share created for the scripts and outputs\n",
    "dotenv.set_key(dotenv_path, 'azure_file_share_name', 'batchaisample')\n",
    "# The Azure file share directory containing the Python scripts\n",
    "dotenv.set_key(dotenv_path, 'script_path', 'scripts')\n",
    "# The script to be run\n",
    "dotenv.set_key(dotenv_path, 'script_name', 'TrainTestClassifier.py')\n",
    "# The Batch AI cluster\n",
    "dotenv.set_key(dotenv_path, 'cluster_name', 'd4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the contents of the `.env` file into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv -o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Python variables used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = os.getenv('configuration_path')\n",
    "azure_blob_container_name = os.getenv('azure_blob_container_name')\n",
    "dataset_path = os.getenv('dataset_path')\n",
    "azure_file_share_name = os.getenv('azure_file_share_name')\n",
    "script_path = os.getenv('script_path')\n",
    "script_name = os.getenv('script_name')\n",
    "cluster_name = os.getenv('cluster_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Batch AI client <a id='client'></a>\n",
    "Read the configuration, and use it to create a Batch AI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "cfg = utils.config.Configuration(configuration_path)\n",
    "client = utils.config.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy training datasets and script to Azure storage <a id='copy'></a>\n",
    "\n",
    "### Azure blob container\n",
    "\n",
    "We create a blob container named `batchaisample` in the storage account for storing the training and testing datasets created in the [data prep notebook](00_Data_Prep.ipynb).\n",
    "\n",
    "**Note** You don't need to create new blob container for every cluster. We are doing this here to simplify resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service = BlockBlobService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "blob_service.create_container(azure_blob_container_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the dataset TSVs to an Azure blob container directory named `dataset` using the Azure SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = glob.glob('*.tsv')\n",
    "for file in dataset_files:\n",
    "    print(file)\n",
    "    blob_service.create_blob_from_path(azure_blob_container_name, \n",
    "                                       dataset_path + '/' + file,\n",
    "                                       file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure file share\n",
    "\n",
    "We create a file share named `batchaisample` in the storage account to hold the training script file created in the [create model notebook](01_Create_Model.ipynb). This will also contain the output files created by the running script.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing here to simplify resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "file_service.create_share(azure_file_share_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training script to file share scripts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_service.create_directory(\n",
    "    azure_file_share_name, script_path, fail_on_exist=False)\n",
    "file_service.create_file_from_path(\n",
    "    azure_file_share_name, script_path, script_name, script_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Azure Batch AI compute cluster <a id='cluster'></a>\n",
    "\n",
    "We will be creating a compute cluster named `d4` with `maximum_node_count` nodes of type `Standard_D4_v2`. We are using auto-scale settings so that the cluster will grow in size to meet the load when we submit jobs. Since you're charged for the Batch AI cluster while the nodes are running, we set the minimum number of nodes to 0 so that once the jobs are done, the cluster will shrink back down. At cluster creation time, one node will be allocated for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_size = 'Standard_D4_v2'\n",
    "maximum_node_count = 16\n",
    "scale_settings = models.ScaleSettings(\n",
    "    auto_scale=models.AutoScaleSettings(minimum_node_count=0,\n",
    "                                        maximum_node_count=maximum_node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together the cluster configuration parameters structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_parameters = models.ClusterCreateParameters(\n",
    "    vm_size=vm_size,\n",
    "    scale_settings=scale_settings,\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password or None,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key or None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cfg.workspace, cluster_name, cluster_parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the just created cluster. The `utilities` module contains a helper function to print out a detailed status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)\n",
    "utils.cluster.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [next notebook](05_Hyperparameter_Search.ipynb), we set up and run the hyperparameter search to tune the parameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MLBatchAIHyperparameterTuning]",
   "language": "python",
   "name": "conda-env-MLBatchAIHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
